---
title: "우울증 분석기: 데이터 수집"
date: 2019-10-30
tags: [machine learning, data science, python]
excerpt: "Machine Learning, RNN, Data Science"
---
# 우울증 분석기  데이터 수집
selenium 을 사용한 네이버 카페 메모 크롤링

1. [설치할 것들 (selenium, chrome webdriver)](#설치할-것들)
2. [크롤링](#크롤링)
3. [다양한 페이지별 크롤링](#다양한-페이지별-크롤링)
* [네이버 카페 메모 클롤링](#메모-크롤링)
* [네이버 카페 게시판 크롤링](#게시판-크롤링)
* [네이버 지식인 크롤링](#네이버-지식인-크롤링)
* [브런치 크롤링](#브런치-크롤링)
2. 데이터 프레임 만들어보기

## 설치할 것들
### beautiful soup
```python
!pip install beautifulsoup4
```
### selenium
```python
!pip install selenium
```
selenium 은 web driver 를 통해 현재 내가 크롤링하는 페이지를 보여주는 기능이 있어서 편리하다
### chrome webdriver
[https://chromedriver.chromium.org/downloads](https://chromedriver.chromium.org/downloads)
크롬 드라이버는 크롬 버전을 확인한 뒤 버전에 맞는것으로 다운받는다.
<img src="{{ site.url }}{{ site.baseurl }}/images/depression/4.jpg" alt="linearly separable data">

위 링크로 들어가 내 크롬 버전에 맞는 크롬 드라이버 설치 파일을 받았다.
<img src="{{ site.url }}{{ site.baseurl }}/images/depression/5.jpg" alt="linearly separable data">
크롬 드라이버를 받아주고 zip파일을 풀어준다

## 크롤링
### package불러오기
selenium 에서 webdriver를 불러온다
```python
from selenium import webdriver
```
크롬 드라이버 파일 위치를 path로 설정하여 webdriver.Chrome명령어로 path 를 열기
```python
path = "D:/사용자/Downloads/chromedriver_win32/chromedriver.exe" #크롬드라이버 파일 위치
driver = webdriver.Chrome(path) #selenium으로 제어할 수 있는 브라우저 새창이 뜬다
```

그 밖의 필요한 패키지들 불러오기
```python
import urllib.request
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import csv
```

### 로그인
```python
driver.implicitly_wait(3)
```
로그인 화면으로 드라이버 주소를 이동시킨다.
```python
# 로그인 전용 화면
driver.get('https://nid.naver.com/nidlogin.login?svctype=262144&url=http://m.naver.com/aside/')
```
아이디와 비밀번호를 입력하고 로그인을 한다.
로그인 버튼의 xpath를 복사한뒤  find_element_by_xpath를 사용해 로그인 버튼을 클릭하였다.
<img src="{{ site.url }}{{ site.baseurl }}/images/depression/6.jpg" alt="linearly separable data">

```python
# 아이디와 비밀번호 입력
driver.find_element_by_name('id').send_keys('****')
driver.find_element_by_name('pw').send_keys('****')
driver.find_element_by_xpath('//*[@id="frmNIDLogin"]/fieldset/input').click()
```
비밀번호와 자동입력방지문자를 다시 입력해야 한다.
```python
# 비밀번호와 자동입력방지문자 입력
driver.find_element_by_name('pw').send_keys('****')
driver.find_element_by_name('chptcha').send_keys('****')

# 로그인 버튼 클릭
driver.find_element_by_xpath('//*[@id="frmNIDLogin"]/fieldset/input').click()
```
크롤링할 페이지의 주소로 간다.
우선 모바일 버전으로 바꾸어서 크롤링하였다.
네이버는 https:// 뒤에 m.을 붙이면 모바일 버전으로 바뀐다.

아래와 같은 코드(driver.get(주소))로 드라이버의 주소를 이동시킨다.
```python
base_url = '크롤링하고 싶은 페이지의 주소' # 우울증 카페 주소
driver.get(base_url)
```

## 다양한 페이지별 크롤링

<img src="{{ site.url }}{{ site.baseurl }}/images/depression/7.jpg" alt="linearly separable data">

### 메모 크롤링
메모 게시판은 더보기 버튼을 클릭해야한다
```python
# 더보기 버튼 20번 클릭
for page_num in range(1,20):
    driver.find_element_by_xpath('//*[@id="memoList"]/div/a').click()
    # 로딩 시간이 있으므로 타이밍 맞추기 위해 sleep(0.5)
    time.sleep(0.5)
```
브라우저 창의 정보를 soup에 저장하고, 글들을 저장한 뒤
글들을 넣기위한 글들의 길이와 같은 데이터프레임을 만들었다
```python
#브라우저 창의 정보를 soup에 저장
soup = BeautifulSoup(driver.page_source , 'html.parser')

#글들을 저장
soup_2 = soup.find('div',class_='u_cbox_list_wrap').find_all('li', class_='u_cbox_comment')

#글들을 넣기위한 데이터프레임 만들기
df = pd.DataFrame(columns=['date', 'name', 'content'])
```

데이터 프레임에 정보를 저장하였다.
```python
# 데이터프레임에 날짜, 아이디, 내용 저장
df = pd.DataFrame(columns=['date', 'name', 'content'], index=range(len(soup_2)))
for i in range(len(soup_2)):
    df.date.loc[i] = soup_2[i].find('span',class_='u_cbox_date').get_text().replace('\xa0',' ')
    name = soup_2[i].find('a',class_='u_cbox_name')
    if name != None:
        df.name.loc[i] = name.get_text().replace('\xa0',' ').replace('\n','. ')
        df.content.loc[i] = soup_2[i].find('span',class_='u_cbox_contents').get_text().replace('\xa0',' ').replace('\n','. ')
    else :
        df = df.drop(i,axis = 0)
```

### 게시판 크롤링
게시판의 경우 위와 같이 더보기 버튼을 통해 크로링할 수 있는 경우가 있었고 없는 경우가 있었다.
더보기 크롤링이 안되는 경우는 아래와 같은 과정을 거쳤다. url의 번호 규칙을 찾아가며 진행했다.
```python
base_url = 'https://m.cafe.naver.com/ArticleList.nhn?search.clubid=13507000&search.menuid=95&search.boardtype=L'
driver.get(base_url)
```
```python
a = 71063
list_str = list(range(a-1200,a))
list_url = ['https://m.cafe.naver.com/ArticleRead.nhn?clubid=13507000&articleid='+str(i)+'&page=1&boardtype=L&menuid=95' for i in list_str]
```

마찬가지로 데이터 프레임에 정보를 저장하였다.
```python
df = pd.DataFrame(columns=['menu','date', 'name', 'title', 'content'], index=range(len(list_url)))
for i in range(len(list_url)):
    try:
        driver.get(list_url[i])
        soup = BeautifulSoup(driver.page_source, "html.parser")
        name = soup.find('h2',class_='tit')
        if name != None:
            df.menu.loc[i]=soup.find('div',class_='tit_menu').get_text().replace('\xa0',' ').replace('\n','')

            df.title.loc[i]=soup.find('h2',class_='tit').get_text().replace('\xa0',' ').replace('\n','. ')

            df.name.loc[i]=soup.find('div',class_='user_wrap').find('a',class_='nick').get_text().replace('\n','')

            date = soup.find('div',class_='user_wrap').find_all('span',class_="date font_l")
            df.date.loc[i] = date[-1].get_text()

            content_list = soup.find('div',class_='post_cont font_zoom1').find_all('p')

            k=''
            for a in content_list[1:]:
                k+=a.get_text().replace('\n','. ').replace('\xa0',' ')

            df.content.loc[i] = k
        else :
            df = df.drop(i,axis = 0)
    except : # chrome alert창 처리해줌
        driver.switch_to_alert.accpet()
        driver.switch_to_alert
        driver.switch_to_alert.accpet()
    time.sleep(0.5)
```
이렇게 크롤링할 경우 특정 게시판이 아닌 카페에 올라온 모든 글들을 크롤링하게 된다.
위 메모 크롤링처럼 더보기를 통해 크롤링하는것은 쉬웠지만 원하는 카페의 게시판은 더보기 버튼이 눌리지 않아 저러한 방식을 사용했다.
그 뒤 menu column에 저장된 게시판 이름을 통해 원하는 게시판의 데이터를 다시 저장하였다.
```python
df1=df1[df1['menu']=='원하는 게시판 명']
```
카페의 원하는 게시판이 정해지지 않은 경우 아래와 같은 과정을 거칠 수 있다.
```python
df.menu.unique()
```
아래 코드를 통해 게시판별 정보 확인
```python
df[df['menu']==list(df.menu.unique())[숫자]]
```
버릴 게시판 리스트를 만들어 진행할 수 있다.
```python
list_drop=[버릴 게시판명 리스트]
df1 = df.copy()
for i in list_drop:
    df1=df1[df1['menu']!=i]
```
### 네이버 지식인 크롤링
저는 우울증입니다 라는 검색을 했을때의 지식인 url들을 가져왔다.
```python
tmp1 = 'https://search.naver.com/search.naver?where=kin'
html = tmp1 + '&sm=tab_jum&ie=utf8&query={key_word}&kin_start={num}'

url = []

for i in range(1,1000,10):

    driver.get(html.format(num = i, key_word = urllib.parse.quote('저는우울증 입니다')))
    article_list = driver.find_elements_by_css_selector('dl a')
    list_url = [ i.get_attribute('href') for i in article_list ]
    url.extend(list_url[-10:])
```
마찬가지로 데이터 프레임에 저장하였다.
```python
df = pd.DataFrame(columns=['title', 'content'], index=range(len(url)))
for i in range(len(url)):
    try:
        response = urlopen(url[i])
        soup = BeautifulSoup(response, "html.parser")
        if soup.find('div',class_='c-heading__content')==None:
            df.content.loc[i] = soup.find('div',class_='c-heading__title').get_text().replace('\t','').replace('\n','')
        else:
            df.title.loc[i] = soup.find('div',class_='c-heading__title').get_text().replace('\t','').replace('\n','')
            df.content.loc[i] = soup.find('div',class_='c-heading__content').get_text().replace('\t','').replace('\n','')[2:]
    except:
        continue
```
### 브런치 크롤링
```python
article_list = driver.find_elements_by_css_selector('li.list_has_image a.link_post')
list_url = [ i.get_attribute('href') for i in article_list ]
df = pd.DataFrame(columns=['content','title','name'], index=range(len(list_url)))
for i in range(len(list_url)):
    try:
        driver.get(list_url[i])
        soup = BeautifulSoup(driver.page_source, "html.parser")
        df.title.loc[i]=soup.find('h1', class_='cover_title').get_text()
        df.name.loc[i]=soup.find('div', class_='wrap_cover').find_all('span')[1].get_text().replace('\n','')
        soup_1 = soup.find_all('p',class_='wrap_item')
        k = ''
        for a in range(len(soup_1)):
            if len(soup_1[a].get_text()) > 0:
                k+=soup_1[a].get_text().replace('\xa0','').replace('\n','. ')
        df.content.loc[i]=k
    except : # chrome alert창 처리해줌
        driver.switch_to_alert.accpet()
        driver.switch_to_alert
        driver.switch_to_alert.accpet()
    time.sleep(0.5)
```
<img src="{{ site.url }}{{ site.baseurl }}/images/depression/8.jpg" alt="linearly separable data">
