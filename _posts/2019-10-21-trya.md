---
title: "대본 쓰는 딥러닝모델"
date: 2019-20-21
tags: [machine learning, data science, python]
excerpt: "Machine Learning, RNN, Data Science"
---

# 드라마 대본 학습시키기 1
우선 드라마 대본 전체를 학습시켜 대본을 쓰는 딥러닝 모델을 만들어 보았다
아래 링크의 투토리얼을 진행하였다.
https://stackabuse.com/text-generation-with-python-and-tensorflow-keras/
1. [준비 작업](#준비-작업)
* [패키지 불러오기](패키지-불러오기)
* [google colab에서 데이터 불러오기](google-colab에서-데이터-불러오기)
2. [데이터 전처리](데이터-전처리)
* [파일의 공백과 특수문자 제거하기](파일의-공백과-특수문자-제거하기)
* [인코딩](인코딩)
* [학습 데이터 만들기](학습-데이터-만들기)
3. [모델 만들기](모델-만들기)
4. [적용시켜보기](적용시켜보기)



## 준비 작업

### 패키지 불러오기
**nltk**를 사용하여 글자 단위로 전처리 하였고
**tensorflow.keras 의 lstm** 모델을 통해 만들었다.


```python
import numpy
import sys
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

from tensorflow.python.keras import utils
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
```


<p style="color: red;">
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>
We recommend you <a href="https://www.tensorflow.org/guide/migrate" target="_blank">upgrade</a> now 
or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:
<a href="https://colab.research.google.com/notebooks/tensorflow_version.ipynb" target="_blank">more info</a>.</p>




```python
import pandas as pd, pickle, re
```

### google colab에서 데이터 불러오기
데이터가 30만개 이상이었기 때문에 gpu를 사용하기 위해 구글 colab으로 진행하였다.


```python
# 구글 드라이브에서 파일을 읽어오기 위해 gauth 인증을 합니다.
# !pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
```

데이터를 불러올때는 드라이브에 있는 파일에 **공유가능 링크**를 만든뒤
바로 ctrl+v를 하면 아래와 같이 urㅣ을 붙여넣기 할 수 있다.
이때 id= 뒷부분을 통해 파일을 불러올 수 있다.


```python
url = 'https://drive.google.com/open?id=1fv_CKCs8tG25gbpW_4emz8RkoN1abn5-'
id = url.split('=')[1]
```


```python
# 데이터의 아이디를 사용해 파일 만들기.
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('finding_love.txt')
```


```python
file = open('finding_love.txt', 'rt', encoding='UTF8').read()
```

## 데이터 전처리

### 파일의 공백과 특수문자 제거하기


```python
def tokenize_words(_input):

    # instantiate the tokenizer
    tokenizer = RegexpTokenizer(r'\w+')
    tokens = tokenizer.tokenize(_input)

    return " ".join(tokens)
```


```python
processed_inputs = tokenize_words(file)
```


```python
processed_inputs[:500]
```




    '연애의 발견 1부 어쩌다가 우리가 한 침대에서 자게 됐어 S 1 어느 피자집 D 10년 전 여름 한눈에 봐도 막 시작한 연인 피자 먹고 있는 태하와 여름 연신 눈을 맞추며 여름 태하씨 10일 남은 거 알아 태하 뭐가 여름 우리 만난지 백일 태하 웃고 왜 받고 싶은 선물 있어 여름 아니 선물은 됐고 태하 여름 주변 슬쩍 돌아보고 몸 앞으로 내밀고 은근하게 우리 그날 호텔가 자 태하 헉 주변 얼른 보고 너 어떻게 그런 말을 이런 데서 여름 태하 반응에 주눅들어 그렇게 조르더니 싫어 태하 말없이 콜라 마시고 여름 싫구나 태하 여름 안 보고 애써 태연한 척 며칠 남았다고 여름 씨익 웃고 딱 열흘 태하 끄덕끄덕 여름 귀엽게 웃고 S 2 호텔 로비 N 10년 전 여름 손잡고 쭈삣쭈삣 걸어오는 태하와 여름 여름 프런트 데스크 보이고 저긴가봐 태하 알아 어제 답사했다니까 여름 주변 두리번 사람들이 우리만 보는 거 같애 태하 조용히 좀 해 둘 프런트 향해 걸어가다가 갑자기 휙 등 돌리고 서는 태하 '



### 인코딩
앞서 만든 인풋 데이터의 글자들에 숫자를 짝지어주는 과정을 거쳤다.


```python
chars = sorted(list(set(processed_inputs)))
char_to_num = dict((c, i) for i, c in enumerate(chars))
```


```python
input_len = len(processed_inputs)
vocab_len = len(chars)
print ("Total number of characters:", input_len)
print ("Total vocab:", vocab_len)
```

    Total number of characters: 380261
    Total vocab: 1324
    

### 학습 데이터 만들기
100글자가 input으로 들어가면 다음 한글자가 나오는 형태로 데이터의 형태를 바꾸기


```python
seq_length = 100
x_data = []
y_data = []
```


```python
# loop through inputs, start at the beginning and go until we hit
# the final character we can create a sequence out of
for i in range(0, input_len - seq_length, 1):
    # Define input and output sequences
    # Input is the current character plus desired sequence length
    in_seq = processed_inputs[i:i + seq_length]

    # Out sequence is the initial character plus total sequence length
    out_seq = processed_inputs[i + seq_length]

    # We now convert list of characters to integers based on
    # previously and add the values to our lists
    x_data.append([char_to_num[char] for char in in_seq])
    y_data.append(char_to_num[out_seq])
```


```python
n_patterns = len(x_data)
print ("Total Patterns:", n_patterns)
```

    Total Patterns: 380161
    

0에서 1사이값으로 만들고 **데이터의 개수(380161) X 시퀀스 길이(100) X 1** 모양으로 데이터 모양을 바꿔주었다.


```python
X = numpy.reshape(x_data, (n_patterns, seq_length, 1))
X = X/float(vocab_len)
```


```python
y = utils.to_categorical(y_data)
```

## 모델 만들기


```python
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(256, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
```

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
    Instructions for updating:
    If using Keras pass *_constraint arguments to layers.
    


```python
model.compile(loss='categorical_crossentropy', optimizer='adam')
```


```python
filepath = "model_weights_saved.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
desired_callbacks = [checkpoint]
```


```python
model.fit(X, y, epochs=10, batch_size=1024, callbacks=desired_callbacks)
```

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    Train on 380161 samples
    Epoch 1/10
    379904/380161 [============================>.] - ETA: 0s - loss: 4.6194
    Epoch 00001: loss improved from inf to 4.61925, saving model to model_weights_saved.hdf5
    380161/380161 [==============================] - 433s 1ms/sample - loss: 4.6192
    Epoch 2/10
    379904/380161 [============================>.] - ETA: 0s - loss: 4.5451
    Epoch 00002: loss improved from 4.61925 to 4.54516, saving model to model_weights_saved.hdf5
    380161/380161 [==============================] - 427s 1ms/sample - loss: 4.5452
    Epoch 3/10
    379904/380161 [============================>.] - ETA: 0s - loss: 4.5430
    Epoch 00003: loss improved from 4.54516 to 4.54301, saving model to model_weights_saved.hdf5
    380161/380161 [==============================] - 427s 1ms/sample - loss: 4.5430
    Epoch 4/10
     99328/380161 [======>.......................] - ETA: 5:14 - loss: 4.5372


```python
model_json = model.to_json()
with open("model.json", "w") as json_file : 
    json_file.write(model_json)
```


```python
filename = "model_weights_saved.hdf5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')
```

## 적용시켜보기


```python
num_to_char = dict((i, c) for i, c in enumerate(chars))
```


```python
start = numpy.random.randint(0, len(x_data) - 1)
pattern = x_data[start]
print("Random Seed:")
print("\"", ''.join([num_to_char[value] for value in pattern]), "\"")
```

    Random Seed:
    "  니다 여름 흑산도 연리지라는 말에 감이 왔고 피식 웃는 솔 짐작도 못하겠다 그런 게 있어 배효원 남직원에게 흑산도 가봤어 남직원 아니라고 윤실장 자 흑산도의 연리지는 과연 무슨  "
    


```python
for i in range(100):
    x = numpy.reshape(pattern, (1, len(pattern), 1))
    x = x / float(vocab_len)
    prediction = model.predict(x, verbose=0)
    index = numpy.argmax(prediction)
    result = num_to_char[index]
    seq_in = [num_to_char[value] for value in pattern]

    sys.stdout.write(result)

    pattern.append(index)
    pattern = pattern[1:len(pattern)]
```

    여름 그 같 같 같 같 같 아 그 아 그 아 그 같 같 아 그 아 그 아 그 같 같 아 그 아 그 아 그 같 같 아 그 아 그 아 그 같 같 아 그 아 그 아 그 같 같 아 그 아
